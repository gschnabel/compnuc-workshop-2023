{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573a2ef4",
   "metadata": {},
   "source": [
    "# Gaussian process regression\n",
    "\n",
    "In this notebook, we are going to implement Gaussian process regression with Tensorflow Probability, which is probabilistic framework built on top of Tensorflow. In addition to automatic differentiation provided by tensorflow, we can easily define probability distributions on variables (and tensors) and connect these variables by functional relationships.\n",
    "\n",
    "First we load all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import numpy as np  # for doing linear algebra stuff\n",
    "import tensorflow as tf  # for doing linear algebra stuff\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b3f1a1",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "With Tensorflow, we can define constants and variables. Constants cannot be changed whereas variables can. They can be defined in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1931455",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1, 2, 3], dtype=tf.float64)\n",
    "x = tf.Variable([4, 5, 6], dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104caed",
   "metadata": {},
   "source": [
    "To avoid cumbersome bugs, it is helpful to be always explicit about the data types by providing the *dtype* argument. Otherwise types will be inferred from the provided list.\n",
    "\n",
    "Inputs can be linked together transformed and linked together. For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tf.add(a, x) # alternatively: r = a + x\n",
    "u = tf.math.square(r)\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a61f6",
   "metadata": {},
   "source": [
    "Reshaping tensors is helpful to bring them to appropriate shape required for an operation. For instance, to convert a one-dimensional tensor to a column vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.reshape(x, (-1, 1))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e9296",
   "metadata": {},
   "source": [
    "The specification of *-1* signifies that tensorflow figures out the appropriate number for that dimension based on the total size of the tensor and the other dimension specifications.\n",
    "\n",
    "To reshape a tensor to obtain a row vector, we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.reshape(x, (1, -1))\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88306344",
   "metadata": {},
   "source": [
    "*Broadcasting* is a useful mechanism to apply mathematical operations on tensor of different shapes. For instance, an element-wise multiplication of a 1x3 tensor with a 3x1 tensor will yield a 3x3 tensor. For instance consider the multiplication of the tensors `a` and `b` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d57889",
   "metadata": {},
   "outputs": [],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387cf351",
   "metadata": {},
   "source": [
    "# Activities\n",
    "\n",
    "A) First we will create functions that will help us to construct the prior distribution, which is a multivariate normal distribution that represents the discretized version of a Gaussian process on a given mesh of x-values. This will involve the following tasks:\n",
    "\n",
    "1) Implementation of a covariance function to calculate a covariance matrix for specified mesh of x-values\n",
    "2) Implementation of a function that creates an instance representing a multivariate normal distribution (part of Tensorflow Probability)\n",
    "3) Then we will use this function to write a function to evaluate the logarithmized probability density function (logpdf) for a given input vector. We refer to this as *prior logpdf*.\n",
    "\n",
    "B) Afterwards, we want to use some observations to constrain the Gaussian process. We are going to implement this in the following way:\n",
    "\n",
    "1) As we have defined the Gaussian process on a discrete mesh of x-values and the observations lie potentially in-between the mesh points, we will implement linear interpolation to map to the observation points.\n",
    "2) Afterwards, using the interpolation function, we are going to write a function to create a function that returns the logarithmized likelihood $\\rho(y_\\text{obs} \\,|\\, y_\\text{mesh})$. This function is the probabilistic link connecting the values at $y_\\text{mesh}$ associated with the Gaussian process prior to the observations.\n",
    "3) Then, we will combine the prior logpdf and the log-likelihood to a joint distribution to obtain the posterior pdf.\n",
    "\n",
    "C) With the posterior pdf available, we will explore a bit how to find the most likely assignment of y-values on the predefined x-mesh. We are going to see how the Adam optimizer performs and then implement one step of Newton method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5f5a2",
   "metadata": {},
   "source": [
    "## Task group (A)\n",
    "\n",
    "### A.1) Implementation of the covariance function:\n",
    "\n",
    "$\\kappa(x, x') = \\delta^2 \\exp\\left(-\\frac{1}{2\\lambda^2} (x_1 - x_2)^2  \\right)$\n",
    "\n",
    "Complete the following function skeleton. `xmesh` is a one-dim vector containing the discretized mesh of x-values for the Gaussian process, `amp` is $\\delta$, `lscl` is $\\lambda$. For numerical stability, a small value is added to the diagonal elements, which is often called a `nugget` parameter.\n",
    "\n",
    "*Hints:*\n",
    "- you may want to reshape `xmesh` at the beginning to be sure about its shape (`tf.reshape`)\n",
    "- Other potential useful functions are `tf.transpose`, `tf.math.square`, `tf.math.exp` \n",
    "- For debugging, instead of directly completing the function body, you may want to create the functionality in a separate cell line by line to check whether inputs and outputs are according to your expectation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_covmat_squaredexp(xmesh, amp, lscl, nugget=1e-6):\n",
    "    # your code here\n",
    "    covmat = covmat + tf.linalg.diag(\n",
    "        tf.fill(tf.size(xmesh), tf.constant(nugget, dtype=tf.float64))\n",
    "    )\n",
    "    return covmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bcbfd0",
   "metadata": {},
   "source": [
    "Here is a brief check whether your implementation is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe42f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testx = tf.constant(np.linspace(0, 10, 10), dtype=tf.float64)\n",
    "testcov = create_covmat_squaredexp(testx, 5, 2, nugget=1e-6)\n",
    "testcov[0:3, 1]\n",
    "# <tf.Tensor: shape=(3,), dtype=float64, numpy=array([21.42492229, 25.000001  , 21.42492229])>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80143775",
   "metadata": {},
   "source": [
    "### A.2) Implementation of prior distribution\n",
    "\n",
    "Now we create a function that should return an object that represents the prior distribution. More precisely, the function should take a covariance matrix and a mean vector and return an instance of `MultivariateNormalTriL`.\n",
    "\n",
    "*Hints:*\n",
    "- In order to produce an instance of `MultivariateNormalTriL`, we need to provide the mean vector as `loc` argument and a Cholesky decomposition of the covariance matrix as argument `scale_tril`.\n",
    "- The function `tf.linalg.cholesky` can be used to compute the Cholesky decomposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1352df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prior_prob(mu, covmat):\n",
    "    # your code here\n",
    "    return mvn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26697934",
   "metadata": {},
   "source": [
    "Let's test the function by using it to create a distribution object and draw samples from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(n, prob): \n",
    "    smpl = prob.sample(n)\n",
    "    for i in range(smpl.shape[0]):\n",
    "        plt.plot(xin, smpl[i, :]) \n",
    "    plt.show()\n",
    "\n",
    "# sample from distribution\n",
    "xin = tf.linspace(0, 10, 100) \n",
    "mu = tf.zeros(xin.shape, dtype=tf.float64) \n",
    "covmat_sqrexp = create_covmat_squaredexp(xin, 2, 2.)\n",
    "prior_prob_sqrexp = create_prior_prob(mu, covmat_sqrexp)\n",
    "plot_samples(5, prior_prob_sqrexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1ad33",
   "metadata": {},
   "source": [
    "### A.3) Implementatin of function to evaluate logarithmized prior pdf\n",
    "\n",
    "Tensorflow probability distribution objects provide the `.log_prob` method. The only thing we need to do is to create a new variable that references this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_logpdf = None # replace None by something here using the prior_prob_sqrexp variable defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87983108",
   "metadata": {},
   "source": [
    "## Task group (B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7bd0e",
   "metadata": {},
   "source": [
    "### B.3: Implementation of linear interpolation\n",
    "\n",
    "In order to obtain predictions at the x-values of the experimental data, we want to use linear interpolation to go from the predefined mesh associated with the Gaussian process to the x-values of the data. The argument `xin` represents the x-mesh associated with the Gaussian process and `yin` the y-values for this mesh. `xout` contains the x-values of interest, i.e., where we have the observations.\n",
    "\n",
    "*Hints:*\n",
    "- You can assume that `xin` is already sorted\n",
    "- The function `tf.searchsorted` is helpful to find into which intervals in `xin` the x-values in `xout` fall.\n",
    "- `tf.gather`, will be helpful to obtain a reduced tensor that is obtained from the original tensor by picking only the values at specific indices.\n",
    "- Once every interval is obtained, you can calculate the `intercept` and `slope` for each interval and use that to calculate the result of the linear interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(xin, yin, xout):\n",
    "    # your code here\n",
    "    return yint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8654e2d",
   "metadata": {},
   "source": [
    "### B.4: Function to return logarithmized likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d51b901",
   "metadata": {},
   "source": [
    "First, let us create some synthetic observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_exp = tf.constant([3, 5], dtype=tf.float64) \n",
    "y_exp = tf.constant([1, 2], dtype=tf.float64)\n",
    "uncs_exp = tf.constant([0.5, 0.9], dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e4ff7",
   "metadata": {},
   "source": [
    "We can plot the Gaussian process prior together with the experimentla data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc829805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experimental_data():\n",
    "    plt.errorbar(x_exp, y_exp, uncs_exp, fmt='o') \n",
    "\n",
    "plot_experimental_data()\n",
    "plot_samples(5, prior_prob_sqrexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9344c",
   "metadata": {},
   "source": [
    "In order to make use of these observations, we want to implement a function that creates a function to evaluate the logarithmied likelihood. You only have to complete the inner function definition. First, we need to obtain based on `yin` on the Gaussian process mesh the predicted values `ymod` at the x-values of the experiment `x_exp`. Finally, we want to create a multivariate normal distribution (`MultivariateNormalDiag`) with a mean vector given by `ymod` and a diagonal covariance matrix with the diagonal elements given by the squared elements in `uncs_exp`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ee205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_likelihood_logpdf(x_exp, y_exp, uncs_exp, xin):\n",
    "    def fun(yin):\n",
    "        # your code here\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bb1e9",
   "metadata": {},
   "source": [
    "Now we can use this function to define the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53457ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "like_logpdf = create_likelihood_logpdf(x_exp, y_exp, uncs_exp, xin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dcb082",
   "metadata": {},
   "source": [
    "We can check if the function works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f6294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "yin_test = tf.constant(np.random.normal(size=xin.shape), dtype=tf.float64)\n",
    "like_logpdf(yin_test)\n",
    "# expected: <tf.Tensor: shape=(), dtype=float64, numpy=-8.140256650466686>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123724ee",
   "metadata": {},
   "source": [
    "### B.3 Log posterior pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a36aa9",
   "metadata": {},
   "source": [
    "Almost done! We now implement a function that returns a function to evaluate the logarithmic posterior distribution. The inner function needs to evaluate the sum of `prior_logpdf` and `like_logpdf` for the passed value `yin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_post_logpdf(prior_logpdf, like_logpdf):\n",
    "    # @tf.function\n",
    "    def fun(yin):\n",
    "        # your code here\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57438b2b",
   "metadata": {},
   "source": [
    "Finally, we can create the function representing the logarithmized posterior pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2160883",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_logpdf = create_post_logpdf(prior_prob_sqrexp.log_prob, like_logpdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab5da7",
   "metadata": {},
   "source": [
    "## Task group (C)\n",
    "\n",
    "We can use optimization to find the most likely values of `yin` that are most compatible with the experimental data. The routines are already provided here. Feel free to try different optimizers or dabble with the observations or Gaussian process prior to see how the posterior predictions change.\n",
    "\n",
    "First, we use the stochastic Adam optimizer and see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_maximizer(x, fun, iters=100, learning_rate=1):\n",
    "    def negfun(x):\n",
    "        return -fun(x)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    for i in range(iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = negfun(x)\n",
    "        gradients = tape.gradient(y, [x])\n",
    "        optimizer.apply_gradients(zip(gradients, [x]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac96f236",
   "metadata": {},
   "source": [
    "Here we study a bit how the Adam optimizer performs. Feel free to adjust parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5390f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yopt = tf.Variable(yin_test)\n",
    "post_logpdf(yopt)\n",
    "adam_maximizer(yopt, post_logpdf, iters=200, learning_rate=0.1)\n",
    "print(post_logpdf(yopt))\n",
    "# plot the solution\n",
    "plot_experimental_data()\n",
    "plt.plot(xin, yopt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ab9bc",
   "metadata": {},
   "source": [
    "In comparison, the following implements the Newton-method to find the root of the first derivative (which means the maximum of the log postpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54573ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs_update(x, logpdf):\n",
    "    with tf.GradientTape() as t2:\n",
    "        t2.watch(x)\n",
    "        with tf.GradientTape() as t1:\n",
    "            t1.watch(x)\n",
    "            y = logpdf(x)\n",
    "        grad_post_logpdf = t1.gradient(y, x)\n",
    "    hessian = t2.jacobian(grad_post_logpdf, x) \n",
    "    # find most likely assignment of parameters\n",
    "    xopt = x - tf.squeeze(tf.linalg.inv(hessian) @ tf.reshape(grad_post_logpdf, (-1, 1)))\n",
    "    # yopt_ref = yin_test - tf.squeeze(tf.linalg.solve(hessian, tf.reshape(grad_post_logpdf, (-1, 1))))\n",
    "    return xopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yopt = bfgs_update(yin_test, post_logpdf)\n",
    "print(post_logpdf(yopt))\n",
    "\n",
    "plot_experimental_data()\n",
    "plt.plot(xin, yopt)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
